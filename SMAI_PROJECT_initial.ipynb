{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhxWGW0txOcS"
   },
   "source": [
    "## IMPORTING LIBRARIES AND DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClOjE9hMxRbX",
    "outputId": "d3b8ce43-6038-41b6-ac93-fed5f6dcf322"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/somyalalwani9/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/somyalalwani9/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/somyalalwani9/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (/home/somyalalwani9/.local/lib/python3.8/site-packages/sklearn/externals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ca9d72cf37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/home/somyalalwani9/.local/lib/python3.8/site-packages/sklearn/externals/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder  #for label encoding\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, MaxPool1D, Dropout, Dense, GlobalMaxPooling1D, Embedding, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "#from google.colab import files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF9xVMLPxTgM"
   },
   "source": [
    "\n",
    "\n",
    "## OPENING TRAIN.CSV & VALID.CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIrNErkwxYmw",
    "outputId": "b32676aa-fd41-41b0-de1e-66c940904497"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def open_filter_data(filename):\n",
    "    \n",
    "    #data = pd.read_csv(\"drive/MyDrive/SMAI_PROJECT_CLICKBAIT/\"+filename+\".csv\", na_values='?', header=None,) \n",
    "    data = pd.read_csv(\"clickbait-news-detection/\"+filename+\".csv\", na_values='?', header=None,) \n",
    "    data.head()\n",
    "    #print(\"data.shape before:\",end=\" \")\n",
    "    #print(data.shape)\n",
    "    \n",
    "    df=data.dropna(axis = 0, how ='any')\n",
    "    #print(\"data.shape after:\",end=\" \")\n",
    "    #print(df.shape)\n",
    "    df=df.reset_index(drop=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "df_train=open_filter_data(\"train\")\n",
    "#df_train.describe\n",
    "\n",
    "#print(\"******\")\n",
    "\n",
    "df_valid=open_filter_data(\"valid\")\n",
    "#df_valid.describe\n",
    "\n",
    "df_test=open_filter_data(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyvJNOeHxb1D"
   },
   "source": [
    "## PREPROCESSING OF TRAINING DATA & VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUPwOBoVxe2p"
   },
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "    a=df.iloc[1:,2]\n",
    "    b=df.iloc[1:,1]\n",
    "    df.iloc[:,3] = df.iloc[:,3].str.replace('news','1')\n",
    "    df.iloc[:,3] = df.iloc[:,3].str.replace('clickbait','0')\n",
    "    y_actual=df.iloc[1:,3]\n",
    "    return a,b,y_actual\n",
    "\n",
    "def extract_dd(df):\n",
    "    a=df.iloc[1:,2]\n",
    "    b=df.iloc[1:,1]\n",
    "    return a,b\n",
    "    \n",
    "\n",
    "a_train,b_train,y_actual=extract_data(df_train)\n",
    "a_valid,b_valid,y_valid=extract_data(df_valid)\n",
    "a_test,b_test=extract_dd(df_test)\n",
    "\n",
    "# print(a)\n",
    "# print(\"**********\")\n",
    "# print(b)\n",
    "# print(\"**********\")\n",
    "# print(y_actual)\n",
    "\n",
    "# print(\"#################\")\n",
    "\n",
    "# print(a_valid)\n",
    "# print(\"**********\")\n",
    "# print(b_valid)\n",
    "# print(\"**********\")\n",
    "# print(y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pel0WQV3xhcn"
   },
   "source": [
    "\n",
    "1. Converting all sentences to lower case \n",
    "2. Writing Abbreviations in full form\n",
    "3. Removing punctuations (Normalisation)\n",
    "4. Removing Stop words (Normalisation)\n",
    "5. Lemmatizing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdQjXuXrxkJN"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
    "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\"i'd\": \"I would\", \"i'd've\": \"I would have\", \"i'll\": \"I will\", \"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\", \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "\"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "\"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why has\", \"why've\": \"why have\",\n",
    "\"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "def pre_process_data(a,b):\n",
    "    final_lines=[]\n",
    "    final_headings=[]\n",
    "\n",
    "\n",
    "    for line in range(1,int(0.25*len(a))):\n",
    "      #print(a[line])\n",
    "      #print(type(a[line]))\n",
    "      #print(\"*********\")\n",
    "      a[line]=(a[line]).lower()     #Converting all sentences to lower case \n",
    "      for word in a[line].split():\n",
    "        if word in contractions:\n",
    "            a[line]=a[line].replace(word, contractions[word.lower()])  #Writing Abbreviations in full form\n",
    "      tokens = word_tokenize(a[line].lower()) \n",
    "      words = [word for word in tokens if word.isalpha()]    #Removing punctuations\n",
    "      final_word = [w for w in words if not w in stop_words]     #Removing Stop words \n",
    "      final_words = [lemmatizer.lemmatize(w) for w in final_word]     #Lemmatizing words\n",
    "      ans=\"\"\n",
    "      for x in final_words:\n",
    "        ans= ans+ \" \"+x\n",
    "      final_lines.append(ans.lstrip())\n",
    "      b[line]=(b[line]).lower()     #Converting all sentences to lower case \n",
    "      for word in b[line].split():\n",
    "        if word in contractions:\n",
    "            b[line]=b[line].replace(word, contractions[word.lower()])  #Writing Abbreviations in full form\n",
    "      tokens = word_tokenize(b[line].lower()) \n",
    "      words = [word for word in tokens if word.isalpha()]    #Removing punctuations\n",
    "      final_head = [w for w in words if not w in stop_words]     #Removing Stop words \n",
    "      final_heads = [lemmatizer.lemmatize(w) for w in final_head]     #Lemmatizing words\n",
    "      ans=\"\"\n",
    "      for x in final_heads:\n",
    "        ans= ans+ \" \"+x\n",
    "      final_headings.append(ans.lstrip())\n",
    "    return final_headings,final_lines\n",
    "\n",
    "\n",
    "def vectorisation(final_lines):    \n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(final_lines)\n",
    "    final_word_vector=[]\n",
    "    i=0\n",
    "    while (i<X.shape[0] and i+10000<X.shape[0]):\n",
    "        final_word_vector.extend(X[i:i+10000].toarray())\n",
    "        i+=10000\n",
    "\n",
    "    final_word_vector.extend(X[i:].toarray())\n",
    "    return final_word_vector\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nju9VLmScvlD"
   },
   "source": [
    "# pre-process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEggNjj9cnQZ"
   },
   "outputs": [],
   "source": [
    "final_headings,final_lines = pre_process_data(a_train,b_train)\n",
    "final_headings_valid,final_body_valid = pre_process_data(a_valid,b_valid)\n",
    "final_headings_test,final_body_test = pre_process_data(a_test,b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSrbq6rtxp3J"
   },
   "outputs": [],
   "source": [
    "# print(len(final_headings),\n",
    "#       len(final_lines),\n",
    "#       len(final_headings_valid),\n",
    "#       len(final_body_valid),\n",
    "#       len(final_headings_test),\n",
    "#       len(final_body_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFNa9M7YdArl"
   },
   "outputs": [],
   "source": [
    "final_headings.extend(final_lines)\n",
    "final_headings.extend(final_headings_valid)\n",
    "final_headings.extend(final_body_valid)\n",
    "final_headings.extend(final_headings_test)\n",
    "final_headings.extend(final_body_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLa9PtZonAdC"
   },
   "outputs": [],
   "source": [
    "del stop_words\n",
    "del lemmatizer\n",
    "del final_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXHhs80uoL9y",
    "outputId": "5893d004-1456-41e0-eab4-c5abab180778"
   },
   "outputs": [],
   "source": [
    "print(len(final_headings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHSLaV6NdMV2"
   },
   "outputs": [],
   "source": [
    "x=vectorisation(final_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19nQB7vR0kjP"
   },
   "outputs": [],
   "source": [
    "#print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A99SsMuT2AR0",
    "outputId": "98ffbc63-c606-46df-8295-de0ad4731b8d"
   },
   "outputs": [],
   "source": [
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9_pA9mJdjUc",
    "outputId": "ab2946dc-0879-4a2b-e557-aac390af2140"
   },
   "outputs": [],
   "source": [
    "final_heading_vector=np.array(x[:4968])\n",
    "joblib.dump(final_heading_vector, 'final_heading_vector.pkl')\n",
    "#del final_heading_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq3z-OMh2TqS"
   },
   "outputs": [],
   "source": [
    "#print(final_heading_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ4dSPfLy3eZ"
   },
   "outputs": [],
   "source": [
    "a=9936\n",
    "\n",
    "final_body_vector=np.array(x[4968:a])\n",
    "#joblib.dump(final_body_vector,  'final_body_vector.pkl')   \n",
    "# del final_body_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ans4N8Jf2SoW"
   },
   "outputs": [],
   "source": [
    "#print(final_body_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic2GB8awy6BP"
   },
   "outputs": [],
   "source": [
    "validation_heading_vector=np.array(x[a:a+705])\n",
    "#joblib.dump(validation_heading_vector,  'validation_heading_vector.pkl')\n",
    "# del validation_heading_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz-v60JI2a4M"
   },
   "outputs": [],
   "source": [
    "#print(validation_heading_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTIX7Teoy7iN"
   },
   "outputs": [],
   "source": [
    "validation_body_vector=np.array(x[a+705:a+1410])\n",
    "#joblib.dump(validation_body_vector,  'validation_body_vector.pkl')\n",
    "# del validation_body_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_xt2sdny9n5"
   },
   "outputs": [],
   "source": [
    "test_heading_vector=np.array(x[a+1410:a+2816])\n",
    "#joblib.dump(test_heading_vector,  'test_heading_vector.pkl')\n",
    "# del test_heading_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jqydpEO2hP6"
   },
   "outputs": [],
   "source": [
    "#print(test_heading_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIy5yLUjy9Zc"
   },
   "outputs": [],
   "source": [
    "test_body_vector=np.array(x[a+2816:])\n",
    "#joblib.dump(test_body_vector, 'test_body_vector.pkl')\n",
    "# del test_body_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUEN5ijV2kEP"
   },
   "outputs": [],
   "source": [
    "#print(test_body_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCoY_a_O0hSA"
   },
   "outputs": [],
   "source": [
    "# print(validation_heading_vector.shape)\n",
    "# print(validation_body_vector.shape)\n",
    "# print(y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww6EJwz-cWPR"
   },
   "source": [
    "# for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d4xokOvb_g4"
   },
   "outputs": [],
   "source": [
    "# print(len(final_heading_vector))\n",
    "# print(len(final_body_vector))\n",
    "# print(len(validation_heading_vector))\n",
    "# print(len(validation_body_vector))\n",
    "# print(len(test_heading_vector))\n",
    "# print(len(test_body_vector))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH-CSyPcXZrF"
   },
   "outputs": [],
   "source": [
    "y_actual=y_actual[:4968].astype(float)\n",
    "y_valid=y_valid[:705].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R14KGhNeYZrX"
   },
   "outputs": [],
   "source": [
    "y_actual=np.array(y_actual)\n",
    "y_valid=np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkOcmX0IrSP1"
   },
   "outputs": [],
   "source": [
    "# joblib.dump(y_actual,  'y_actual.pkl')\n",
    "# #del y_actual\n",
    "\n",
    "# joblib.dump(y_valid,  'y_valid.pkl')\n",
    "# #del y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nZ3AlaAx4ow"
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seTkJCjnrdHb"
   },
   "outputs": [],
   "source": [
    "# pickle.load('final_heading_vector.pkl')\n",
    "final_body_vector=joblib.load('final_body_vector.pkl')\n",
    "validation_heading_vector=joblib.load('validation_heading_vector.pkl')\n",
    "validation_body_vector=joblib.load('validation_body_vector.pkl')\n",
    "test_heading_vector=joblib.load('test_heading_vector.pkl')\n",
    "test_body_vector=joblib.load('test_body_vector.pkl')\n",
    "y_actual=joblib.load('y_actual.pkl')\n",
    "y_valid=joblib.load('y_valid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJpy96kon6eO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w77iqDBLUDh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_base_network(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='relu')(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
    "    # x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    \n",
    "    margin = 1\n",
    "    sqaure_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzgaL_qZz4Wx"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qgIfpo10CSp"
   },
   "outputs": [],
   "source": [
    "# print(validation_heading_vector.shape)\n",
    "# print(validation_body_vector.shape)\n",
    "# print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RpizOxqMEZN"
   },
   "outputs": [],
   "source": [
    "# network definition\n",
    "input_shape = final_heading_vector.shape[1:]\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "#rms = Adam()\n",
    "#rms = SGD()\n",
    "\n",
    "model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "# train\n",
    "history = model.fit([final_heading_vector, final_body_vector], y_actual,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=([validation_heading_vector,validation_body_vector], y_valid))\n",
    "\n",
    "y_pred = model.predict([test_heading_vector, test_heading_vector])\n",
    "#tr_acc = compute_accuracy(y_test, y_pred_tr)\n",
    "#te_acc = compute_accuracy(y_test, y_pred_tr)\n",
    "\n",
    "#print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "#print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SMAI PROJECT final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
