{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMAI_CLICKBAIT_DETECTION_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEb6hPs91Fy"
      },
      "source": [
        "# **Similarity Based Bi-Directional LSTM Model for Clickbait Detection**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP-6rfrw-FaC"
      },
      "source": [
        "**SUBMITTED BY :**\n",
        "\n",
        "              1. NAMAN JUNEJA (2020201072)\n",
        "              2. SOMYA LALWANI (2020201092)\n",
        "              3. PULLAMMA MAYAKUNTALA (2018101119)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmPA12vw--2b"
      },
      "source": [
        "## IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClOjE9hMxRbX",
        "outputId": "4a8527ee-afda-4e68-a76e-7cd13e3c8c6d"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder  #for label encoding\n",
        "from sklearn.metrics.cluster import homogeneity_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "from tensorflow.keras.layers import LSTM, MaxPool1D, Dropout, Dense, GlobalMaxPooling1D, Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from keras.backend import int_shape\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (56.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF9xVMLPxTgM"
      },
      "source": [
        "\n",
        "\n",
        "## OPENING TRAIN.CSV & VALID.CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIrNErkwxYmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133e0298-c056-46d5-da42-8700b813d32c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def open_filter_data(filename):\n",
        "    \n",
        "    data = pd.read_csv(\"drive/MyDrive/SMAI_PROJECT_CLICKBAIT/\"+filename+\".csv\", na_values='?', header=None,) \n",
        "    #data = pd.read_csv(\"clickbait-news-detection/\"+filename+\".csv\", na_values='?', header=None,) \n",
        "    data.head()\n",
        "    data=data.replace(r'^\\s+$', np.nan, regex=True)\n",
        "    df=data.dropna(axis = 0, how ='any')\n",
        "    #df=df.reset_index(drop=True)\n",
        "    return df\n",
        "    \n",
        "\n",
        "df_train=open_filter_data(\"train\")\n",
        "df_valid=open_filter_data(\"valid\")\n",
        "df_test=open_filter_data(\"test\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUayAkjmb8h_"
      },
      "source": [
        "index_names = df_train [df_train.iloc[:,3] == 'other' ].index\n",
        "#print(type(index_names))\n",
        "df_train=df_train.drop(index_names, inplace = False)\n",
        "df_train=df_train.reset_index(drop=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWBpZjweiwd8"
      },
      "source": [
        "index_names = df_valid [df_valid.iloc[:,3] == 'other' ].index\n",
        "#print(type(index_names))\n",
        "df_valid=df_valid.drop(index_names, inplace = False)\n",
        "df_valid=df_valid.reset_index(drop=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCkw93hpkWiR"
      },
      "source": [
        "df_test=df_test.reset_index(drop=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuA4wECsAfCq"
      },
      "source": [
        "## Training data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN1rg7WyADY4",
        "outputId": "e2e7411d-db5c-4b2d-dc67-38fd40c54539"
      },
      "source": [
        "df_train.describe"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of            0  ...          3\n",
              "0         id  ...      label\n",
              "1          0  ...       news\n",
              "2          1  ...       news\n",
              "3          2  ...  clickbait\n",
              "4          3  ...  clickbait\n",
              "...      ...  ...        ...\n",
              "18326  24863  ...       news\n",
              "18327  24867  ...       news\n",
              "18328  24868  ...       news\n",
              "18329  24869  ...       news\n",
              "18330  24870  ...       news\n",
              "\n",
              "[18331 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0AVekCJAskd"
      },
      "source": [
        "## Validation data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArenUrSjAKhY",
        "outputId": "a254ad8a-adde-44ba-9b42-8b940c43d231"
      },
      "source": [
        "df_valid.describe"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of          0  ...      3\n",
              "0       id  ...  label\n",
              "1        0  ...   news\n",
              "2        1  ...   news\n",
              "3        2  ...   news\n",
              "4        3  ...   news\n",
              "...    ...  ...    ...\n",
              "2620  3544  ...   news\n",
              "2621  3545  ...   news\n",
              "2622  3547  ...   news\n",
              "2623  3550  ...   news\n",
              "2624  3551  ...   news\n",
              "\n",
              "[2625 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-AVb3E0Av57"
      },
      "source": [
        "## Testing data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WRd-uErAMuI",
        "outputId": "f9fa1761-50bd-4dee-9383-b91cda21ae69"
      },
      "source": [
        "df_test.describe"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of          0  ...                                                  2\n",
              "0       id  ...                                               text\n",
              "1        0  ...  More Try Yahoo Finance on Firefox » Amazon CEO...\n",
              "2        1  ...  More Laura Dern seems to be everywhere these d...\n",
              "3        2  ...  Kirkuk is a city of Northern Iraq in the Kurdi...\n",
              "4        3  ...  Experts say that communication is the cornerst...\n",
              "...    ...  ...                                                ...\n",
              "5625  5642  ...  “Watch out boy, she’ll chew you up.” On this d...\n",
              "5626  5643  ...  Fire broke out at an upmarket apartment block ...\n",
              "5627  5644  ...  For the first time, a database has compiled a ...\n",
              "5628  5645  ...  Vin Diesel has sometimes been known to jump th...\n",
              "5629  5646  ...  In three days, Donald Trump is all set to assu...\n",
              "\n",
              "[5630 rows x 3 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyvJNOeHxb1D"
      },
      "source": [
        "## EXTRACTING COLUMNS FROM TRAINING DATA & VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUPwOBoVxe2p"
      },
      "source": [
        "def extract_data(df):\n",
        "    a=df.iloc[1:,2]\n",
        "    b=df.iloc[1:,1]\n",
        "    df.iloc[:,3] = df.iloc[:,3].str.replace('news','1')\n",
        "    df.iloc[:,3] = df.iloc[:,3].str.replace('clickbait','0')\n",
        "    y_actual=df.iloc[1:,3]\n",
        "    return a,b,y_actual\n",
        "\n",
        "def extract_dd(df):\n",
        "    a=df.iloc[1:,2]\n",
        "    b=df.iloc[1:,1]\n",
        "    return a,b\n",
        "    \n",
        "\n",
        "a_train,b_train,y_actual=extract_data(df_train)\n",
        "a_valid,b_valid,y_valid=extract_data(df_valid)\n",
        "a_test,b_test=extract_dd(df_test)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH-BG9hE_iEg",
        "outputId": "26db980b-3ea8-4cd0-b287-121254407e4c"
      },
      "source": [
        "print(\"TRAINING DATA ---->\")\n",
        "print()\n",
        "print(\"Heading---->\")\n",
        "print()\n",
        "print(a_train[:5])\n",
        "print()\n",
        "print(\"Body--->\")\n",
        "print()\n",
        "print(b_train[:5])\n",
        "print()\n",
        "print(\"Y---->\")\n",
        "print(\"1 means News & 0 means Clickbait\")\n",
        "print()\n",
        "print(y_actual[:5])\n",
        "\n",
        "print()\n",
        "print(\"Similar for validation & testing data\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING DATA ---->\n",
            "\n",
            "Heading---->\n",
            "\n",
            "1    Economists generally agree: China must overhau...\n",
            "2    LONDON—British Prime Minister Theresa May said...\n",
            "3    Beaches come in all sorts of shapes and sizes ...\n",
            "4    A timeline of what happened after Tamir Rice, ...\n",
            "5    An Italian neurosurgeon who has claimed for mo...\n",
            "Name: 2, dtype: object\n",
            "\n",
            "Body--->\n",
            "\n",
            "1    China and Economic Reform: Xi Jinping’s Track ...\n",
            "2    Trade to Be a Big Topic in Theresa May’s U.S. ...\n",
            "3    The Top Beaches In The World, According To Nat...\n",
            "4    Sheriff’s Report Provides New Details on Tamir...\n",
            "5    Surgeon claiming he will transplant volunteer'...\n",
            "Name: 1, dtype: object\n",
            "\n",
            "Y---->\n",
            "1 means News & 0 means Clickbait\n",
            "\n",
            "1    1\n",
            "2    1\n",
            "3    0\n",
            "4    0\n",
            "5    1\n",
            "Name: 3, dtype: object\n",
            "\n",
            "Similar for validation & testing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nju9VLmScvlD"
      },
      "source": [
        "## Preprocess All Data (Training, Validation, Testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pel0WQV3xhcn"
      },
      "source": [
        "\n",
        "1. Converting all sentences to lower case \n",
        "2. Writing Abbreviations in full form\n",
        "3. Removing punctuations (Normalisation)\n",
        "4. Removing Stop words (Normalisation)\n",
        "5. Lemmatizing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdQjXuXrxkJN"
      },
      "source": [
        "contractions = {\n",
        "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
        "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\"i'd\": \"I would\", \"i'd've\": \"I would have\", \"i'll\": \"I will\", \"i'll've\": \"I will have\",\n",
        "\"i'm\": \"I am\", \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that had\",\n",
        "\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
        "\"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "\"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why has\", \"why've\": \"why have\",\n",
        "\"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "\n",
        "def pre_process_data(a,b):\n",
        "    final_lines=[]\n",
        "    final_headings=[]\n",
        "\n",
        "\n",
        "    for line in range(1,int(len(a)+1)):\n",
        "      #print(a[line])\n",
        "      #print(type(a[line]))\n",
        "      #print(\"*********\")\n",
        "      a[line]=(a[line]).lower()     #Converting all sentences to lower case \n",
        "      for word in a[line].split():\n",
        "        if word in contractions:\n",
        "            a[line]=a[line].replace(word, contractions[word.lower()])  #Writing Abbreviations in full form\n",
        "      tokens = word_tokenize(a[line].lower()) \n",
        "      words = [word for word in tokens if word.isalpha()]    #Removing punctuations\n",
        "      final_word = [w for w in words if not w in stop_words]     #Removing Stop words \n",
        "      final_words = [lemmatizer.lemmatize(w) for w in final_word]     #Lemmatizing words\n",
        "      ans=\"\"\n",
        "      for x in final_words:\n",
        "        ans= ans+ \" \"+x\n",
        "      final_lines.append(ans.lstrip())\n",
        "      b[line]=(b[line]).lower()     #Converting all sentences to lower case \n",
        "      for word in b[line].split():\n",
        "        if word in contractions:\n",
        "            b[line]=b[line].replace(word, contractions[word.lower()])  #Writing Abbreviations in full form\n",
        "      tokens = word_tokenize(b[line].lower()) \n",
        "      words = [word for word in tokens if word.isalpha()]    #Removing punctuations\n",
        "      final_head = [w for w in words if not w in stop_words]     #Removing Stop words \n",
        "      final_heads = [lemmatizer.lemmatize(w) for w in final_head]     #Lemmatizing words\n",
        "      ans=\"\"\n",
        "      for x in final_heads:\n",
        "        ans= ans+ \" \"+x\n",
        "      final_headings.append(ans.lstrip())\n",
        "    return final_headings,final_lines\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrnKsSYylAj9"
      },
      "source": [
        "final_headings,final_lines = pre_process_data(a_train,b_train)\n",
        "final_headings_valid,final_body_valid = pre_process_data(a_valid,b_valid)\n",
        "final_headings_test,final_body_test = pre_process_data(a_test,b_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqMxoANgG1j",
        "outputId": "ecbfe326-e3fa-45fb-e897-8cfed2959548"
      },
      "source": [
        "print(len(a_train),len(b_train),len(y_actual))\n",
        "print(len(a_valid),len(b_valid),len(y_valid))\n",
        "print(len(a_test),len(b_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18330 18330 18330\n",
            "2624 2624 2624\n",
            "5629 5629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql0yEOf2gyCh",
        "outputId": "d0d909a2-570b-4419-c259-2fde6ae07c48"
      },
      "source": [
        "print(len(final_headings))\n",
        "print(len(final_lines))\n",
        "print(len(final_headings_valid))\n",
        "print(len(final_body_valid))\n",
        "print(len(final_headings_test))\n",
        "print(len(final_body_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18330\n",
            "18330\n",
            "2624\n",
            "2624\n",
            "5629\n",
            "5629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcdMkGSZB8Pc"
      },
      "source": [
        "## CONVERTING SENTENCES TO VECTORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wmIob6FKqh8"
      },
      "source": [
        "def vectorisation(final_lines):    \n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(final_lines)\n",
        "    final_word_vector=[]\n",
        "    i=0\n",
        "    while (i<X.shape[0] and i+9000<X.shape[0]):\n",
        "        final_word_vector.extend(X[i:i+9000].toarray())\n",
        "        i+=9000\n",
        "\n",
        "    final_word_vector.extend(X[i:].toarray())\n",
        "    return final_word_vector\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFNa9M7YdArl"
      },
      "source": [
        "final_headings.extend(final_lines)\n",
        "final_headings.extend(final_headings_valid)\n",
        "final_headings.extend(final_body_valid)\n",
        "final_headings.extend(final_headings_test)\n",
        "final_headings.extend(final_body_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLa9PtZonAdC"
      },
      "source": [
        "del stop_words\n",
        "del lemmatizer\n",
        "del final_lines"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0OCe3EewZz6"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(final_headings)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHSLaV6NdMV2"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "model = Doc2Vec(documents, vector_size=50, window=2, min_count=1, workers=4)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV2rYbnww8zO",
        "outputId": "97577ecc-6f26-4d99-c5ca-f4723f9c6f06"
      },
      "source": [
        "print(model.docvecs[10])\n",
        "x=[]\n",
        "for i in range(len(model.docvecs)):\n",
        "  x.append(model.docvecs[i])\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.00598366  0.00333252 -0.00424861 -0.00211288 -0.00961624  0.00779518\n",
            " -0.00532748  0.00809509  0.00180822 -0.00936934 -0.00845145  0.00584467\n",
            "  0.00056669 -0.00601255 -0.00935218 -0.00534277  0.00766556 -0.0091926\n",
            "  0.00035213  0.00821604  0.00120993 -0.00998544  0.00544651 -0.00185315\n",
            "  0.00881828  0.00051547 -0.00520432 -0.00678589 -0.00721345  0.00070543\n",
            " -0.00948756  0.00611665 -0.00612757  0.00294123  0.00807801  0.00821683\n",
            "  0.00953233  0.0043191  -0.00844529 -0.00941301  0.00868589 -0.00386174\n",
            "  0.00323217  0.00940358 -0.00671448  0.00503128 -0.00826502  0.00569582\n",
            " -0.00072017  0.00428437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnGQSl1hySTc",
        "outputId": "9d31a24e-8759-4c68-cedb-6d2bb878f4ce"
      },
      "source": [
        "x=np.array(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(53166, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCR0FxO6xcuv",
        "outputId": "29bad86b-de81-42f5-b101-77f39926372b"
      },
      "source": [
        "print(model.docvecs[0])\n",
        "print(x[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00834881 -0.09048623 -0.0498955   0.08682026  0.02324477  0.0357379\n",
            "  0.14426142  0.02104693 -0.00749789 -0.01978051  0.01015609  0.00908074\n",
            "  0.03031624 -0.08752505  0.08681321 -0.05033393 -0.0401216   0.1076416\n",
            "  0.01115645 -0.02351255  0.01328233  0.01773491  0.00744983 -0.04728574\n",
            " -0.03551555 -0.02138336  0.02685723 -0.03599524  0.0327532  -0.00621227\n",
            " -0.08505448 -0.10805422 -0.03214097 -0.00756444  0.0803344   0.03748083\n",
            " -0.01389184 -0.08793445 -0.02179187 -0.06300236 -0.02621869 -0.00280195\n",
            " -0.00208306  0.04794564  0.01144013  0.07794328 -0.05426467 -0.00339033\n",
            "  0.03940855 -0.04464821]\n",
            "[-0.00834881 -0.09048623 -0.0498955   0.08682026  0.02324477  0.0357379\n",
            "  0.14426142  0.02104693 -0.00749789 -0.01978051  0.01015609  0.00908074\n",
            "  0.03031624 -0.08752505  0.08681321 -0.05033393 -0.0401216   0.1076416\n",
            "  0.01115645 -0.02351255  0.01328233  0.01773491  0.00744983 -0.04728574\n",
            " -0.03551555 -0.02138336  0.02685723 -0.03599524  0.0327532  -0.00621227\n",
            " -0.08505448 -0.10805422 -0.03214097 -0.00756444  0.0803344   0.03748083\n",
            " -0.01389184 -0.08793445 -0.02179187 -0.06300236 -0.02621869 -0.00280195\n",
            " -0.00208306  0.04794564  0.01144013  0.07794328 -0.05426467 -0.00339033\n",
            "  0.03940855 -0.04464821]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A99SsMuT2AR0",
        "outputId": "0f31c9b1-e8f7-4734-bf6e-eb80e2991449"
      },
      "source": [
        "print(type(x))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9_pA9mJdjUc"
      },
      "source": [
        "final_heading_vector=np.array(x[0:18330])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ4dSPfLy3eZ"
      },
      "source": [
        "a=18330*2\n",
        "final_body_vector=np.array(x[18330:a])\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic2GB8awy6BP"
      },
      "source": [
        "validation_heading_vector=np.array(x[a:a+2624])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTIX7Teoy7iN"
      },
      "source": [
        "validation_body_vector=np.array(x[a+2624:a+5248])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xt2sdny9n5"
      },
      "source": [
        "test_heading_vector=np.array(x[a+5248:a+5248+5629])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIy5yLUjy9Zc"
      },
      "source": [
        "test_body_vector=np.array(x[a+5248+5629:])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH-CSyPcXZrF"
      },
      "source": [
        "y_actual=y_actual.astype(float)\n",
        "y_valid=y_valid.astype(float)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R14KGhNeYZrX"
      },
      "source": [
        "y_actual=np.array(y_actual)\n",
        "y_valid=np.array(y_valid)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41mmhUH0CU3Y"
      },
      "source": [
        "## PRINTING SIZE OF ALL VECTORS FORMED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qgIfpo10CSp",
        "outputId": "be83caae-a229-4c64-a745-c75ec3215780"
      },
      "source": [
        "print(final_heading_vector.shape)\n",
        "print(final_body_vector.shape)\n",
        "print(y_actual.shape)\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "print(validation_heading_vector.shape)\n",
        "print(validation_body_vector.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "print(test_heading_vector.shape)\n",
        "print(test_body_vector.shape)\n",
        "print(\"----------------------\")\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18330, 50)\n",
            "(18330, 50)\n",
            "(18330,)\n",
            "----------------------\n",
            "(2624, 50)\n",
            "(2624, 50)\n",
            "(2624,)\n",
            "----------------------\n",
            "(5629, 50)\n",
            "(5629, 50)\n",
            "----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nZ3AlaAx4ow"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w77iqDBLUDh"
      },
      "source": [
        "def create_base_network(input_shape):\n",
        "    input = Input(shape=(input_shape[0],input_shape[1]))\n",
        "    \n",
        "    x =Bidirectional(LSTM(32,activation='relu',return_sequences=True))(input)\n",
        "    x =LSTM(32,activation='sigmoid')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
        "    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
        "    \n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    \n",
        "    margin = 1.0\n",
        "    sqaure_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return float(K.mean(float(y_true) * float(sqaure_pred) + (1.0 - float(y_true)) * float(margin_square)))\n",
        "\n",
        "\n",
        "def cosine_sim(vects):\n",
        "  a,b=vects\n",
        "  ans=[]\n",
        "  s=int_shape(a)[1]\n",
        "  for i in range(s):\n",
        "    temp1=a[i]\n",
        "    temp2=b[i]\n",
        "    normalize_a = tf.nn.l2_normalize(temp1,0)        \n",
        "    normalize_b = tf.nn.l2_normalize(temp2,0)\n",
        "    cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
        "    ans.append(cos_similarity)\n",
        "  return tf.convert_to_tensor(ans)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htstl6bCkSkR"
      },
      "source": [
        "#conveting to 3d data for model\n",
        "fh=final_heading_vector.reshape((final_heading_vector.shape[0],final_heading_vector.shape[1],1))\n",
        "fb=final_body_vector.reshape((final_body_vector.shape[0],final_body_vector.shape[1],1))\n",
        "fh=fh[:18304]\n",
        "fb=fb[:18304]\n",
        "\n",
        "f1=validation_heading_vector.reshape((validation_heading_vector.shape[0],validation_heading_vector.shape[1],1))\n",
        "f2=validation_body_vector.reshape((validation_body_vector.shape[0],validation_body_vector.shape[1],1))\n",
        "\n",
        "y_actual=y_actual.reshape((y_actual.shape[0],1))\n",
        "y_actual=y_actual[:18304]\n",
        "y_valid=y_valid.reshape((y_valid.shape[0],1))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsyYFipqkJS3"
      },
      "source": [
        "# network definition\n",
        "input_shape = final_heading_vector.shape[1]\n",
        "ipt_shape=(final_heading_vector.shape[1],1)\n",
        "base_network = create_base_network(ipt_shape)\n",
        "\n",
        "input_a = Input(shape=ipt_shape)\n",
        "input_b = Input(shape=ipt_shape)\n",
        "\n",
        "# because we re-use the same instance `base_network`,\n",
        "# the weights of the network\n",
        "# will be shared across the two branches\n",
        "\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = Lambda(cosine_sim)([processed_a, processed_b])\n",
        "\n",
        "model = Model([input_a, input_b], distance)\n",
        "\n",
        "opt = keras.optimizers.Adam()\n",
        "\n",
        "earlystop = EarlyStopping(patience=5)\n",
        " \n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.2, min_lr=0.001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "model.compile(loss=contrastive_loss, optimizer=opt)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RpizOxqMEZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3103468-6ad1-4415-d610-8448fd1e7695"
      },
      "source": [
        "history = model.fit([fh, fb], y_actual,\n",
        "          batch_size=32,\n",
        "          epochs=100,callbacks=callbacks,\n",
        "          validation_data=([f1,f2], y_valid))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "572/572 [==============================] - 48s 67ms/step - loss: 0.6505 - val_loss: 0.3240\n",
            "Epoch 2/100\n",
            "572/572 [==============================] - 37s 64ms/step - loss: 0.1805 - val_loss: 0.1943\n",
            "Epoch 3/100\n",
            "572/572 [==============================] - 37s 64ms/step - loss: 0.1772 - val_loss: 0.1704\n",
            "Epoch 4/100\n",
            "572/572 [==============================] - 37s 65ms/step - loss: 0.1755 - val_loss: 0.1698\n",
            "Epoch 5/100\n",
            "572/572 [==============================] - 37s 64ms/step - loss: 0.1685 - val_loss: 0.1709\n",
            "Epoch 6/100\n",
            "572/572 [==============================] - 37s 65ms/step - loss: 0.1709 - val_loss: 0.1704\n",
            "Epoch 7/100\n",
            "572/572 [==============================] - 37s 64ms/step - loss: 0.1707 - val_loss: 0.1695\n",
            "Epoch 8/100\n",
            "572/572 [==============================] - 37s 64ms/step - loss: 0.1706 - val_loss: 0.1858\n",
            "Epoch 9/100\n",
            "572/572 [==============================] - 37s 65ms/step - loss: 0.1887 - val_loss: 0.1727\n",
            "Epoch 10/100\n",
            "572/572 [==============================] - 37s 65ms/step - loss: 0.1764 - val_loss: 0.1731\n",
            "Epoch 11/100\n",
            "572/572 [==============================] - 37s 65ms/step - loss: 0.1707 - val_loss: 0.1720\n",
            "Epoch 12/100\n",
            "572/572 [==============================] - 37s 65ms/step - loss: 0.1721 - val_loss: 0.1710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofaP4Gwch6bT"
      },
      "source": [
        "from keras.models import load_model\n",
        "model.save('model_smai_project1.h5')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4o2NE6yHKkY"
      },
      "source": [
        "## SUMMARY OF MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvLgKY_xz5HX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4773e34e-e589-447f-d473-42df5048e51f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 50, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 50, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Functional)              (None, 32)           21120       input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (32,)                0           model[0][0]                      \n",
            "                                                                 model[1][0]                      \n",
            "==================================================================================================\n",
            "Total params: 21,120\n",
            "Trainable params: 21,120\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNObeBXXG_TW"
      },
      "source": [
        "## PREDICTING Y FOR VALIDATION DATA AND CHECKING ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhamfeUhbyQS"
      },
      "source": [
        "y_pred = model.predict([f1,f2])\n",
        "y_valid=y_valid"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NlNcgg4Gjah"
      },
      "source": [
        "for i in range(len(y_pred)):\n",
        "  if(y_pred[i]<0.5):\n",
        "    y_pred[i]=1\n",
        "  else:\n",
        "    y_pred[i]=0"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNCFJxfEGUiE",
        "outputId": "9c6549af-8ba2-4d99-cbff-5142904f0040"
      },
      "source": [
        "print(\"SHAPE OF Y PREDICTED : \")\n",
        "print(y_pred.shape)\n",
        "print()\n",
        "print(\"SOME Y PREDICTED VALUES:\")\n",
        "print(y_pred[:15])\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE OF Y PREDICTED : \n",
            "(2624,)\n",
            "\n",
            "SOME Y PREDICTED VALUES:\n",
            "[1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHkGtFPMIaJI",
        "outputId": "7ddbb041-6adf-421d-f00a-b05c1ca5eb5f"
      },
      "source": [
        "acc=accuracy_score(y_valid, y_pred)\n",
        "print(\"ACCURACY SCORE FOR VALIDATION DATA = \",acc*100)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE FOR VALIDATION DATA =  78.8109756097561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YYqBSRoH4wx",
        "outputId": "6cfcbbbb-aec8-47f2-c0f6-a44603284920"
      },
      "source": [
        "print(\"CLASSIFICATION REPORT FOR TESTING DATA : \")\n",
        "print()\n",
        "print(classification_report(y_valid,y_pred))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLASSIFICATION REPORT FOR TESTING DATA : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.33      0.03      0.05       539\n",
            "         1.0       0.80      0.98      0.88      2085\n",
            "\n",
            "    accuracy                           0.79      2624\n",
            "   macro avg       0.56      0.51      0.47      2624\n",
            "weighted avg       0.70      0.79      0.71      2624\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0-IwsYcbJcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "03bf01e3-d027-4824-f6cc-e45fc70347dc"
      },
      "source": [
        "plt.scatter(range(20),y_pred[:20],c=\"r\")\n",
        "plt.scatter(range(20),y_valid[:20],c=\"g\")\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqWLNrz5GMQN"
      },
      "source": [
        "## PREDICTING Y FOR TESTING DATA (1 FOR NEWS , 0 FOR CLICKBAIT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn7qnSeBHbXx"
      },
      "source": [
        "fh=test_heading_vector.reshape((test_heading_vector.shape[0],test_heading_vector.shape[1],1))\n",
        "fb=test_body_vector.reshape((test_body_vector.shape[0],test_body_vector.shape[1],1))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpaEDPCUHnNT"
      },
      "source": [
        "y_pred = model.predict([f1,f2])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQK4TwALHnWs"
      },
      "source": [
        "for i in range(len(y_pred)):\n",
        "  if(y_pred[i]<0.5):\n",
        "    y_pred[i]=1\n",
        "  else:\n",
        "    y_pred[i]=0"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5HIc4IsHnWt",
        "outputId": "a1e09139-b806-4c23-9eb2-3cb0d387de0b"
      },
      "source": [
        "print(\"SHAPE OF Y PREDICTED : \")\n",
        "print(y_pred.shape)\n",
        "print()\n",
        "print(\"Y PREDICTED VALUES FOR TESTING DATA (1 MEANS NEWS, 0 MEANS CLICKBAIT):\")\n",
        "print()\n",
        "print(y_pred)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE OF Y PREDICTED : \n",
            "(2624,)\n",
            "\n",
            "Y PREDICTED VALUES FOR TESTING DATA (1 MEANS NEWS, 0 MEANS CLICKBAIT):\n",
            "\n",
            "[1. 0. 0. ... 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}