{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIDIRECTIONAL_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEb6hPs91Fy"
      },
      "source": [
        "# **Similarity Based Bi-Directional LSTM Model for Clickbait Detection**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP-6rfrw-FaC"
      },
      "source": [
        "**SUBMITTED BY :**\n",
        "\n",
        "              1. NAMAN JUNEJA (2020201072)\n",
        "              2. SOMYA LALWANI (2020201092)\n",
        "              3. PULLAMMA MAYAKUNTALA (2018101119)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmPA12vw--2b"
      },
      "source": [
        "## IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClOjE9hMxRbX",
        "outputId": "e792e434-9fd7-4811-d6df-007b7342de23"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder  #for label encoding\n",
        "from sklearn.metrics.cluster import homogeneity_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "from tensorflow.keras.layers import LSTM, MaxPool1D, Dropout, Dense, GlobalMaxPooling1D, Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from keras.backend import int_shape\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (54.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF9xVMLPxTgM"
      },
      "source": [
        "\n",
        "\n",
        "## OPENING TRAIN.CSV & VALID.CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIrNErkwxYmw",
        "outputId": "fba81b57-ac53-4deb-9697-77d72b2a307e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def open_filter_data(filename):\n",
        "    \n",
        "    data = pd.read_csv(\"drive/MyDrive/SMAI_PROJECT_CLICKBAIT/\"+filename+\".csv\", na_values='?', header=None,) \n",
        "    #data = pd.read_csv(\"clickbait-news-detection/\"+filename+\".csv\", na_values='?', header=None,) \n",
        "    data.head()\n",
        "    df=data.dropna(axis = 0, how ='any')\n",
        "    #print(df.shape)\n",
        "    df=df.reset_index(drop=True)\n",
        "    return df\n",
        "    \n",
        "\n",
        "df_train=open_filter_data(\"train\")\n",
        "df_valid=open_filter_data(\"valid\")\n",
        "df_test=open_filter_data(\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuA4wECsAfCq"
      },
      "source": [
        "## Training data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN1rg7WyADY4",
        "outputId": "af7a0117-7d6c-4f8b-ef6a-d3f374d03feb"
      },
      "source": [
        "df_train.describe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of            0  ...          3\n",
              "0         id  ...      label\n",
              "1          0  ...       news\n",
              "2          1  ...       news\n",
              "3          2  ...  clickbait\n",
              "4          3  ...  clickbait\n",
              "...      ...  ...        ...\n",
              "19873  24864  ...      other\n",
              "19874  24867  ...       news\n",
              "19875  24868  ...       news\n",
              "19876  24869  ...       news\n",
              "19877  24870  ...       news\n",
              "\n",
              "[19878 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0AVekCJAskd"
      },
      "source": [
        "## Validation data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArenUrSjAKhY",
        "outputId": "bb03f2c2-212f-4145-9dd4-1f54a3a09ee9"
      },
      "source": [
        "df_valid.describe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of          0  ...      3\n",
              "0       id  ...  label\n",
              "1        0  ...   news\n",
              "2        1  ...   news\n",
              "3        2  ...   news\n",
              "4        3  ...   news\n",
              "...    ...  ...    ...\n",
              "2820  3544  ...   news\n",
              "2821  3545  ...   news\n",
              "2822  3547  ...   news\n",
              "2823  3550  ...   news\n",
              "2824  3551  ...   news\n",
              "\n",
              "[2825 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-AVb3E0Av57"
      },
      "source": [
        "## Testing data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WRd-uErAMuI",
        "outputId": "47a3d26b-3fd9-4329-cd54-9deadf9fb939"
      },
      "source": [
        "df_test.describe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of          0  ...                                                  2\n",
              "0       id  ...                                               text\n",
              "1        0  ...  More Try Yahoo Finance on Firefox » Amazon CEO...\n",
              "2        1  ...  More Laura Dern seems to be everywhere these d...\n",
              "3        2  ...  Kirkuk is a city of Northern Iraq in the Kurdi...\n",
              "4        3  ...  Experts say that communication is the cornerst...\n",
              "...    ...  ...                                                ...\n",
              "5625  5642  ...  “Watch out boy, she’ll chew you up.” On this d...\n",
              "5626  5643  ...  Fire broke out at an upmarket apartment block ...\n",
              "5627  5644  ...  For the first time, a database has compiled a ...\n",
              "5628  5645  ...  Vin Diesel has sometimes been known to jump th...\n",
              "5629  5646  ...  In three days, Donald Trump is all set to assu...\n",
              "\n",
              "[5630 rows x 3 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyvJNOeHxb1D"
      },
      "source": [
        "## EXTRACTING COLUMNS FROM TRAINING DATA & VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUPwOBoVxe2p"
      },
      "source": [
        "def extract_data(df):\n",
        "    a=df.iloc[1:,2]\n",
        "    b=df.iloc[1:,1]\n",
        "    df.iloc[:,3] = df.iloc[:,3].str.replace('news','1')\n",
        "    df.iloc[:,3] = df.iloc[:,3].str.replace('clickbait','0')\n",
        "    y_actual=df.iloc[1:,3]\n",
        "    return a,b,y_actual\n",
        "\n",
        "def extract_dd(df):\n",
        "    a=df.iloc[1:,2]\n",
        "    b=df.iloc[1:,1]\n",
        "    return a,b\n",
        "    \n",
        "\n",
        "a_train,b_train,y_actual=extract_data(df_train)\n",
        "a_valid,b_valid,y_valid=extract_data(df_valid)\n",
        "a_test,b_test=extract_dd(df_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH-BG9hE_iEg",
        "outputId": "ba8a68a4-12fc-41e9-cef1-7c7e71d8ddd3"
      },
      "source": [
        "print(\"TRAINING DATA ---->\")\n",
        "print()\n",
        "print(\"Heading---->\")\n",
        "print()\n",
        "print(a_train[:5])\n",
        "print()\n",
        "print(\"Body--->\")\n",
        "print()\n",
        "print(b_train[:5])\n",
        "print()\n",
        "print(\"Y---->\")\n",
        "print(\"1 means News & 0 means Clickbait\")\n",
        "print()\n",
        "print(y_actual[:5])\n",
        "\n",
        "print()\n",
        "print(\"Similar for validation & testing data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING DATA ---->\n",
            "\n",
            "Heading---->\n",
            "\n",
            "1    Economists generally agree: China must overhau...\n",
            "2    LONDON—British Prime Minister Theresa May said...\n",
            "3    Beaches come in all sorts of shapes and sizes ...\n",
            "4    A timeline of what happened after Tamir Rice, ...\n",
            "5    An Italian neurosurgeon who has claimed for mo...\n",
            "Name: 2, dtype: object\n",
            "\n",
            "Body--->\n",
            "\n",
            "1    China and Economic Reform: Xi Jinping’s Track ...\n",
            "2    Trade to Be a Big Topic in Theresa May’s U.S. ...\n",
            "3    The Top Beaches In The World, According To Nat...\n",
            "4    Sheriff’s Report Provides New Details on Tamir...\n",
            "5    Surgeon claiming he will transplant volunteer'...\n",
            "Name: 1, dtype: object\n",
            "\n",
            "Y---->\n",
            "1 means News & 0 means Clickbait\n",
            "\n",
            "1    1\n",
            "2    1\n",
            "3    0\n",
            "4    0\n",
            "5    1\n",
            "Name: 3, dtype: object\n",
            "\n",
            "Similar for validation & testing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nju9VLmScvlD"
      },
      "source": [
        "## Preprocess All Data (Training, Validation, Testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pel0WQV3xhcn"
      },
      "source": [
        "\n",
        "1. Converting all sentences to lower case \n",
        "2. Writing Abbreviations in full form\n",
        "3. Removing punctuations (Normalisation)\n",
        "4. Removing Stop words (Normalisation)\n",
        "5. Lemmatizing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdQjXuXrxkJN"
      },
      "source": [
        "contractions = {\n",
        "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
        "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\"i'd\": \"I would\", \"i'd've\": \"I would have\", \"i'll\": \"I will\", \"i'll've\": \"I will have\",\n",
        "\"i'm\": \"I am\", \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that had\",\n",
        "\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
        "\"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "\"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why has\", \"why've\": \"why have\",\n",
        "\"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "\n",
        "def pre_process_data(a,b):\n",
        "    final_lines=[]\n",
        "    final_headings=[]\n",
        "\n",
        "\n",
        "    for line in range(1,int(0.25*len(a))):\n",
        "      #print(a[line])\n",
        "      #print(type(a[line]))\n",
        "      #print(\"*********\")\n",
        "      a[line]=(a[line]).lower()     #Converting all sentences to lower case \n",
        "      for word in a[line].split():\n",
        "        if word in contractions:\n",
        "            a[line]=a[line].replace(word, contractions[word.lower()])  #Writing Abbreviations in full form\n",
        "      tokens = word_tokenize(a[line].lower()) \n",
        "      words = [word for word in tokens if word.isalpha()]    #Removing punctuations\n",
        "      final_word = [w for w in words if not w in stop_words]     #Removing Stop words \n",
        "      final_words = [lemmatizer.lemmatize(w) for w in final_word]     #Lemmatizing words\n",
        "      ans=\"\"\n",
        "      for x in final_words:\n",
        "        ans= ans+ \" \"+x\n",
        "      final_lines.append(ans.lstrip())\n",
        "      b[line]=(b[line]).lower()     #Converting all sentences to lower case \n",
        "      for word in b[line].split():\n",
        "        if word in contractions:\n",
        "            b[line]=b[line].replace(word, contractions[word.lower()])  #Writing Abbreviations in full form\n",
        "      tokens = word_tokenize(b[line].lower()) \n",
        "      words = [word for word in tokens if word.isalpha()]    #Removing punctuations\n",
        "      final_head = [w for w in words if not w in stop_words]     #Removing Stop words \n",
        "      final_heads = [lemmatizer.lemmatize(w) for w in final_head]     #Lemmatizing words\n",
        "      ans=\"\"\n",
        "      for x in final_heads:\n",
        "        ans= ans+ \" \"+x\n",
        "      final_headings.append(ans.lstrip())\n",
        "    return final_headings,final_lines\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEggNjj9cnQZ"
      },
      "source": [
        "final_headings,final_lines = pre_process_data(a_train,b_train)\n",
        "final_headings_valid,final_body_valid = pre_process_data(a_valid,b_valid)\n",
        "final_headings_test,final_body_test = pre_process_data(a_test,b_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcdMkGSZB8Pc"
      },
      "source": [
        "## CONVERTING SENTENCES TO VECTORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wmIob6FKqh8"
      },
      "source": [
        "def vectorisation(final_lines):    \n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(final_lines)\n",
        "    final_word_vector=[]\n",
        "    i=0\n",
        "    while (i<X.shape[0] and i+9000<X.shape[0]):\n",
        "        final_word_vector.extend(X[i:i+9000].toarray())\n",
        "        i+=9000\n",
        "\n",
        "    final_word_vector.extend(X[i:].toarray())\n",
        "    return final_word_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFNa9M7YdArl"
      },
      "source": [
        "final_headings.extend(final_lines)\n",
        "final_headings.extend(final_headings_valid)\n",
        "final_headings.extend(final_body_valid)\n",
        "final_headings.extend(final_headings_test)\n",
        "final_headings.extend(final_body_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLa9PtZonAdC"
      },
      "source": [
        "del stop_words\n",
        "del lemmatizer\n",
        "del final_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0OCe3EewZz6"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(final_headings)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHSLaV6NdMV2"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "model = Doc2Vec(documents, vector_size=50, window=2, min_count=1, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV2rYbnww8zO",
        "outputId": "bfcb915b-f4e7-4fe9-86c6-d2c997d4e290"
      },
      "source": [
        "print(model.docvecs[10])\n",
        "x=[]\n",
        "for i in range(len(model.docvecs)):\n",
        "  x.append(model.docvecs[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00943957 -0.00565967 -0.00056115  0.00371834  0.0091646   0.00073052\n",
            " -0.00286133  0.00862753 -0.00224862 -0.00258791  0.00696541 -0.0038803\n",
            "  0.00423511  0.00080352  0.00360002 -0.00692782  0.00581241 -0.00739102\n",
            " -0.00123478  0.00667639 -0.00862207  0.00659981  0.0081884   0.00952029\n",
            "  0.00760527 -0.00395039 -0.00879464 -0.00452148 -0.00913566 -0.00815774\n",
            " -0.00813918 -0.00806905 -0.0017303   0.00916989 -0.00575324  0.00833617\n",
            "  0.00317828  0.0001753  -0.00962291 -0.00924913  0.00725003  0.00139232\n",
            "  0.00370185 -0.00059981 -0.00221023  0.00740702  0.00498748 -0.00969003\n",
            "  0.00149805  0.00295465]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnGQSl1hySTc",
        "outputId": "3b0d0927-9fec-49be-abd8-647541158c1a"
      },
      "source": [
        "x=np.array(x)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14158, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCR0FxO6xcuv",
        "outputId": "9ed4d223-3944-429c-ebc3-f842da3cd39b"
      },
      "source": [
        "print(model.docvecs[0])\n",
        "print(x[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.03536771  0.02000762 -0.06072801 -0.0816667   0.08619923  0.08203556\n",
            " -0.00799381  0.01304136  0.1004592  -0.0120927   0.0533181  -0.03688552\n",
            "  0.01851783 -0.02818307  0.00820541 -0.05661358  0.05425253 -0.09424765\n",
            "  0.00066803  0.02000421 -0.01107238 -0.01065262  0.04465072  0.0459039\n",
            " -0.0506875  -0.01661285  0.05960898  0.00270901  0.09889501 -0.12814616\n",
            "  0.02683649  0.05196254  0.03601348  0.10297931  0.0696995  -0.00163625\n",
            "  0.01419099  0.03035801  0.00573389 -0.0610261  -0.0197304  -0.10483076\n",
            "  0.03264156 -0.07126989 -0.06606618  0.03374589  0.10452054 -0.07979453\n",
            "  0.02127404  0.00998791]\n",
            "[-0.03536771  0.02000762 -0.06072801 -0.0816667   0.08619923  0.08203556\n",
            " -0.00799381  0.01304136  0.1004592  -0.0120927   0.0533181  -0.03688552\n",
            "  0.01851783 -0.02818307  0.00820541 -0.05661358  0.05425253 -0.09424765\n",
            "  0.00066803  0.02000421 -0.01107238 -0.01065262  0.04465072  0.0459039\n",
            " -0.0506875  -0.01661285  0.05960898  0.00270901  0.09889501 -0.12814616\n",
            "  0.02683649  0.05196254  0.03601348  0.10297931  0.0696995  -0.00163625\n",
            "  0.01419099  0.03035801  0.00573389 -0.0610261  -0.0197304  -0.10483076\n",
            "  0.03264156 -0.07126989 -0.06606618  0.03374589  0.10452054 -0.07979453\n",
            "  0.02127404  0.00998791]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A99SsMuT2AR0",
        "outputId": "bddc4651-8904-4c30-d890-406249524a18"
      },
      "source": [
        "print(type(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9_pA9mJdjUc"
      },
      "source": [
        "final_heading_vector=np.array(x[0:4968])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ4dSPfLy3eZ"
      },
      "source": [
        "a=9936\n",
        "final_body_vector=np.array(x[4968:a])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic2GB8awy6BP"
      },
      "source": [
        "validation_heading_vector=np.array(x[a:a+705])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTIX7Teoy7iN"
      },
      "source": [
        "validation_body_vector=np.array(x[a+705:a+1410])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xt2sdny9n5"
      },
      "source": [
        "test_heading_vector=np.array(x[a+1410:a+2816])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIy5yLUjy9Zc"
      },
      "source": [
        "test_body_vector=np.array(x[a+2816:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH-CSyPcXZrF"
      },
      "source": [
        "y_actual=y_actual[:4968].astype(float)\n",
        "y_valid=y_valid[:705].astype(float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R14KGhNeYZrX"
      },
      "source": [
        "y_actual=np.array(y_actual)\n",
        "y_valid=np.array(y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41mmhUH0CU3Y"
      },
      "source": [
        "## PRINTING SIZE OF ALL VECTORS FORMED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qgIfpo10CSp",
        "outputId": "6ed41ae3-0afa-4562-ea27-75df583cb5a4"
      },
      "source": [
        "print(final_heading_vector.shape)\n",
        "print(final_body_vector.shape)\n",
        "print(y_actual.shape)\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "print(validation_heading_vector.shape)\n",
        "print(validation_body_vector.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "print(test_heading_vector.shape)\n",
        "print(test_body_vector.shape)\n",
        "print(\"----------------------\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4968, 50)\n",
            "(4968, 50)\n",
            "(4968,)\n",
            "----------------------\n",
            "(705, 50)\n",
            "(705, 50)\n",
            "(705,)\n",
            "----------------------\n",
            "(1406, 50)\n",
            "(1406, 50)\n",
            "----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nZ3AlaAx4ow"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w77iqDBLUDh"
      },
      "source": [
        "def create_base_network(input_shape):\n",
        "    input = Input(shape=(input_shape[0],input_shape[1]))\n",
        "    \n",
        "    x =Bidirectional(LSTM(32,activation='relu',return_sequences=True))(input)\n",
        "    x =LSTM(32,activation='sigmoid')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
        "    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
        "    \n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    \n",
        "    margin = 1.0\n",
        "    sqaure_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return float(K.mean(float(y_true) * float(sqaure_pred) + (1.0 - float(y_true)) * float(margin_square)))\n",
        "\n",
        "\n",
        "def cosine_sim(vects):\n",
        "  a,b=vects\n",
        "  ans=[]\n",
        "  s=int_shape(a)[1]\n",
        "  for i in range(s):\n",
        "    temp1=a[i]\n",
        "    temp2=b[i]\n",
        "    normalize_a = tf.nn.l2_normalize(temp1,0)        \n",
        "    normalize_b = tf.nn.l2_normalize(temp2,0)\n",
        "    cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
        "    ans.append(cos_similarity)\n",
        "  return tf.convert_to_tensor(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RpizOxqMEZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1a8f4e-5f1f-461d-c022-86b990420615"
      },
      "source": [
        "# network definition\n",
        "input_shape = final_heading_vector.shape[1]\n",
        "ipt_shape=(final_heading_vector.shape[1],1)\n",
        "base_network = create_base_network(ipt_shape)\n",
        "\n",
        "input_a = Input(shape=ipt_shape)\n",
        "input_b = Input(shape=ipt_shape)\n",
        "\n",
        "# because we re-use the same instance `base_network`,\n",
        "# the weights of the network\n",
        "# will be shared across the two branches\n",
        "\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = Lambda(cosine_sim)([processed_a, processed_b])\n",
        "\n",
        "model = Model([input_a, input_b], distance)\n",
        "\n",
        "opt = keras.optimizers.Adam()\n",
        "\n",
        "earlystop = EarlyStopping(patience=5)\n",
        " \n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.2, min_lr=0.001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "model.compile(loss=contrastive_loss, optimizer=opt)\n",
        "\n",
        "#conveting to 3d data for model\n",
        "fh=final_heading_vector.reshape((final_heading_vector.shape[0],final_heading_vector.shape[1],1))\n",
        "fb=final_body_vector.reshape((final_body_vector.shape[0],final_body_vector.shape[1],1))\n",
        "\n",
        "f1=validation_heading_vector.reshape((validation_heading_vector.shape[0],validation_heading_vector.shape[1],1))\n",
        "f2=validation_body_vector.reshape((validation_body_vector.shape[0],validation_body_vector.shape[1],1))\n",
        "\n",
        "y_actual=y_actual.reshape((y_actual.shape[0],1))\n",
        "y_valid=y_valid.reshape((y_valid.shape[0],1))\n",
        "\n",
        "\n",
        "history = model.fit([fh[:4928], fb[:4928]], y_actual[:4928],\n",
        "          batch_size=32,\n",
        "          epochs=50,callbacks=callbacks,\n",
        "          validation_data=([f1[:704],f2[:704]], y_valid[:704]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "154/154 [==============================] - 21s 78ms/step - loss: 0.6089 - val_loss: 0.7429\n",
            "Epoch 2/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.5923 - val_loss: 0.2327\n",
            "Epoch 3/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.2223 - val_loss: 0.2170\n",
            "Epoch 4/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.2026 - val_loss: 0.2334\n",
            "Epoch 5/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.2111 - val_loss: 0.2026\n",
            "Epoch 6/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.2059 - val_loss: 0.2029\n",
            "Epoch 7/50\n",
            "154/154 [==============================] - 10s 68ms/step - loss: 0.2130 - val_loss: 0.2021\n",
            "Epoch 8/50\n",
            "154/154 [==============================] - 10s 68ms/step - loss: 0.2057 - val_loss: 0.2017\n",
            "Epoch 9/50\n",
            "154/154 [==============================] - 10s 68ms/step - loss: 0.2061 - val_loss: 0.2077\n",
            "Epoch 10/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.2095 - val_loss: 0.2024\n",
            "Epoch 11/50\n",
            "154/154 [==============================] - 10s 67ms/step - loss: 0.2033 - val_loss: 0.2057\n",
            "Epoch 12/50\n",
            "154/154 [==============================] - 10s 68ms/step - loss: 0.2026 - val_loss: 0.2021\n",
            "Epoch 13/50\n",
            "154/154 [==============================] - 11s 69ms/step - loss: 0.2868 - val_loss: 0.2031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4o2NE6yHKkY"
      },
      "source": [
        "## SUMMARY OF MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvLgKY_xz5HX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c8d967-2c67-4963-91a4-a2ab28ac885c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           [(None, 50, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_15 (InputLayer)           [(None, 50, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_8 (Functional)            (None, 32)           21120       input_14[0][0]                   \n",
            "                                                                 input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (32,)                0           model_8[0][0]                    \n",
            "                                                                 model_8[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 21,120\n",
            "Trainable params: 21,120\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNObeBXXG_TW"
      },
      "source": [
        "## PREDICTING Y FOR VALIDATION DATA AND CHECKING ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhamfeUhbyQS"
      },
      "source": [
        "y_pred = model.predict([f1[:704],f2[:704]])\n",
        "y_valid=y_valid[:704]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NlNcgg4Gjah"
      },
      "source": [
        "for i in range(len(y_pred)):\n",
        "  if(y_pred[i]<0.5):\n",
        "    y_pred[i]=1\n",
        "  else:\n",
        "    y_pred[i]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNCFJxfEGUiE",
        "outputId": "3805a947-e486-476e-ecf5-33b1e042bab5"
      },
      "source": [
        "print(\"SHAPE OF Y PREDICTED : \")\n",
        "print(y_pred.shape)\n",
        "print()\n",
        "print(\"SOME Y PREDICTED VALUES:\")\n",
        "print(y_pred[:15])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE OF Y PREDICTED : \n",
            "(704,)\n",
            "\n",
            "SOME Y PREDICTED VALUES:\n",
            "[1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHkGtFPMIaJI",
        "outputId": "f0b1674b-8fc5-4442-e586-901c7ac2cec3"
      },
      "source": [
        "acc=accuracy_score(y_valid, y_pred)\n",
        "print(\"ACCURACY SCORE FOR VALIDATION DATA = \",acc*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE FOR VALIDATION DATA =  73.29545454545455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YYqBSRoH4wx",
        "outputId": "9ba2e87f-210a-49ff-e791-1351dbf0954d"
      },
      "source": [
        "print(\"CLASSIFICATION REPORT FOR TESTING DATA : \")\n",
        "print()\n",
        "print(classification_report(y_valid,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLASSIFICATION REPORT FOR TESTING DATA : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.32      0.03      0.06       181\n",
            "         1.0       0.74      0.98      0.84       523\n",
            "\n",
            "    accuracy                           0.73       704\n",
            "   macro avg       0.53      0.50      0.45       704\n",
            "weighted avg       0.63      0.73      0.64       704\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0-IwsYcbJcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "17a10ad9-888e-4daf-9ed5-bb91eb585550"
      },
      "source": [
        "plt.scatter(range(20),y_pred[:20],c=\"r\")\n",
        "plt.scatter(range(20),y_valid[:20],c=\"g\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqWLNrz5GMQN"
      },
      "source": [
        "## PREDICTING Y FOR TESTING DATA (1 FOR NEWS , 0 FOR CLICKBAIT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn7qnSeBHbXx"
      },
      "source": [
        "fh=test_heading_vector.reshape((test_heading_vector.shape[0],test_heading_vector.shape[1],1))\n",
        "fb=test_body_vector.reshape((test_body_vector.shape[0],test_body_vector.shape[1],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpaEDPCUHnNT"
      },
      "source": [
        "y_pred = model.predict([f1[:704],f2[:704]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQK4TwALHnWs"
      },
      "source": [
        "for i in range(len(y_pred)):\n",
        "  if(y_pred[i]<0.5):\n",
        "    y_pred[i]=1\n",
        "  else:\n",
        "    y_pred[i]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5HIc4IsHnWt",
        "outputId": "53e01bff-eaeb-441c-b44d-88e11bc72b4b"
      },
      "source": [
        "print(\"SHAPE OF Y PREDICTED : \")\n",
        "print(y_pred.shape)\n",
        "print()\n",
        "print(\"Y PREDICTED VALUES FOR TESTING DATA (1 MEANS NEWS, 0 MEANS CLICKBAIT):\")\n",
        "print()\n",
        "print(y_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE OF Y PREDICTED : \n",
            "(704,)\n",
            "\n",
            "Y PREDICTED VALUES FOR TESTING DATA (1 MEANS NEWS, 0 MEANS CLICKBAIT):\n",
            "\n",
            "[1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}